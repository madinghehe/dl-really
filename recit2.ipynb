{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear (256 -> 128)\n",
       "  (fc2): Linear (128 -> 64)\n",
       "  (fc3): Linear (64 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=16*4*4, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        #multiply the dimensions except first\n",
    "        sz = x.size()[1:]\n",
    "        s = reduce(lambda p, q : p*q, sz)\n",
    "        #print('s', s)\n",
    "        return s\n",
    "    \n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "Files already downloaded\n",
      "train size: 60000, 28, 28\n",
      "('one batch images size:', torch.Size([4, 1, 28, 28]))\n",
      "('labels:', \n",
      " 3\n",
      " 3\n",
      " 5\n",
      " 6\n",
      "[torch.LongTensor of size 4]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+H8oNJbjCnKcbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10de8e4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#MNIST data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./mnist', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./mnist', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=4, num_workers=2)\n",
    "\n",
    "print('train size: %d, %d, %d' % trainset.train_data.size())\n",
    "print('one batch images size:', images.size())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "    img = img/2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images, padding=1, nrow=4))\n",
    "#print('labels:', labels)\n",
    "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len of params: ', 10)\n",
      "('size of ', 0, ' params: ', torch.Size([6, 1, 5, 5]))\n",
      "('size of ', 1, ' params: ', torch.Size([6]))\n",
      "('size of ', 2, ' params: ', torch.Size([16, 6, 5, 5]))\n",
      "('size of ', 3, ' params: ', torch.Size([16]))\n",
      "('size of ', 4, ' params: ', torch.Size([128, 256]))\n",
      "('size of ', 5, ' params: ', torch.Size([128]))\n",
      "('size of ', 6, ' params: ', torch.Size([64, 128]))\n",
      "('size of ', 7, ' params: ', torch.Size([64]))\n",
      "('size of ', 8, ' params: ', torch.Size([10, 64]))\n",
      "('size of ', 9, ' params: ', torch.Size([10]))\n",
      "[    0     0] loss: 0.000\n",
      "('conv1 bias grad', Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -0.5132 -0.3480  0.5658  0.8013  0.8184\n",
      "  -1.1286 -0.9824  0.0692  0.8672  0.9121\n",
      "  -0.7323 -0.5038 -0.0347  0.4920  0.4094\n",
      "  -0.1709 -0.4424 -0.1981  0.2579  0.2253\n",
      "  -0.3020 -0.3969 -0.1768 -0.2382  0.0665\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "   2.1214  2.1474  2.2677  0.6894  0.1448\n",
      "   2.0182  2.3694  1.9089  0.4018 -0.1687\n",
      "   2.2005  1.9924  0.6993 -0.5022 -0.1594\n",
      "   2.1508  1.0663  1.1278  0.5548  0.8932\n",
      "   1.4196  1.3653  1.3949  1.7552  1.3243\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -1.1078 -2.1580 -3.1788 -2.6748 -2.9332\n",
      "  -0.6554 -1.0586 -1.3220 -1.2673 -1.8695\n",
      "   1.1847  0.4240  0.4530 -0.1203 -0.2769\n",
      "   1.9668  1.0355 -0.3793 -0.7259 -0.7905\n",
      "  -0.3269 -1.8849 -2.2327 -1.8792 -1.7234\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -0.4251 -0.5788 -0.5154 -0.0726  0.1458\n",
      "  -0.4207 -0.4869 -0.4168  0.4204  0.9463\n",
      "  -0.2746 -0.3063  0.1050  0.4583  1.0083\n",
      "  -0.3321 -0.3710 -0.2211  0.2061  0.7606\n",
      "  -0.2246 -0.5187 -0.5242 -0.2864  0.2295\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -1.3946 -1.5685 -1.8157 -1.5940 -0.3075\n",
      "  -0.2705 -0.8606 -0.4054 -0.4576  0.8812\n",
      "   0.7088  0.9769  0.7664  0.4626  0.5823\n",
      "   0.9587  0.6727  0.2098 -0.3227 -0.4556\n",
      "   1.0853  1.4287  0.4867  0.1301 -0.0708\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -0.1831 -0.2188  0.2955 -0.2406 -0.8931\n",
      "   1.0354  1.5647  1.7104  0.8896 -0.1910\n",
      "   1.5553  1.3656  0.6739 -0.5346 -1.3276\n",
      "   0.5947  0.5695  0.2834 -0.4189 -1.3410\n",
      "   0.1815  0.0446  0.1719 -0.4553 -0.9568\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      ")\n",
      "[    0  2000] loss: 0.015\n",
      "('conv1 bias grad', Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "   0.0060  0.2444  0.6019  0.3970 -0.4048\n",
      "   0.5754  0.7457  0.8059  0.1854 -0.7095\n",
      "   0.6866  0.6760  0.5533 -0.4953 -1.3334\n",
      "   0.6818  0.7506  0.4579 -0.7700 -1.0578\n",
      "   0.6775  0.6135  0.2796 -0.8554 -0.6792\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "   0.2731  1.1624  0.7447  0.5352 -0.7556\n",
      "  -1.1837 -1.4918 -1.3250 -0.4510 -0.3883\n",
      "  -1.1657  0.2955  2.0013  2.9995  2.4828\n",
      "   0.5266  2.6494  3.2425  3.7964  3.5974\n",
      "   2.0266  3.4873  3.9924  4.0840  3.7316\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "  -1.5118 -1.5324 -1.9159 -2.7695 -4.6810\n",
      "  -0.0905  0.0777 -0.0768 -1.6854 -4.1553\n",
      "   0.6528  0.8712  0.9730 -1.0746 -2.8066\n",
      "   2.0511  2.7493  3.1758  0.1186 -1.2245\n",
      "   3.6042  4.3591  3.6470 -0.2837 -1.2090\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "   0.2832  0.2376  0.2759 -0.6026 -2.1806\n",
      "   1.0389  1.2858  1.1988 -0.0062 -1.9092\n",
      "   1.7046  1.6429  1.4997 -0.0163 -1.5383\n",
      "   2.1184  2.1251  1.9587  0.4788 -1.0313\n",
      "   2.3306  2.2326  2.0307  0.4034 -0.6717\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "   0.4006 -0.2021 -0.3580 -0.1656 -0.3317\n",
      "   0.5634 -0.0739 -0.0744  0.1331 -0.1121\n",
      "   0.3157 -0.1331  0.0355  0.1617 -0.1257\n",
      "  -0.4241 -0.6767 -0.4348 -0.0037 -0.3571\n",
      "  -1.2747 -1.2846 -1.3493 -0.7376 -0.9517\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-05 *\n",
      "  -2.1699 -2.2340 -1.2781  0.2288  0.3831\n",
      "  -3.7019 -3.2700 -1.3165  1.2291  1.3605\n",
      "  -3.4244 -3.0707 -0.0450  2.6794  0.5356\n",
      "  -4.1908 -3.6733 -0.7298  0.6585 -2.0614\n",
      "  -4.7367 -2.8334 -0.3488  0.2931 -2.5156\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      ")\n",
      "[    0  4000] loss: 0.015\n",
      "('conv1 bias grad', Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "   0.3790  0.4641  0.6271  0.6827  0.4353\n",
      "   0.7006  0.7591  0.7663  0.2858 -0.2256\n",
      "   0.8186  1.0731  0.8533  0.0946 -0.7633\n",
      "   0.7472  1.1539  0.5935 -0.5298 -1.6529\n",
      "   0.6228  0.7378  0.3690 -0.6593 -1.4405\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -1.5796 -1.3124 -0.3268  0.3553  0.6297\n",
      "  -1.0413 -0.6444  0.0583  0.0842 -0.2546\n",
      "  -0.0009  0.3389  0.0379 -0.0132 -0.6127\n",
      "   0.2583  0.1693 -0.1875 -0.2255 -0.9524\n",
      "  -0.4933 -0.3755 -0.3300 -0.5877 -0.8801\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "   2.0268  2.1253  1.5539  0.4481 -0.3171\n",
      "   1.9191  2.0318  1.0681 -0.0288 -1.2448\n",
      "   1.1537  1.0589 -0.0881 -1.0291 -1.0806\n",
      "   0.9211  0.1187 -0.6316 -1.1756 -0.7014\n",
      "   0.8524  0.6148  0.1240 -0.0342  0.7144\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "   0.0931  0.2216  0.2463  0.2198  0.3340\n",
      "   0.4508  0.5974  0.5364  0.4350  0.2047\n",
      "   0.7142  0.8234  0.7854  0.7418  0.5323\n",
      "   0.7721  0.8963  0.8588  0.5992  0.1542\n",
      "   1.0278  0.9881  0.7439  0.5263  0.1719\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "   0.7777  0.8196  1.0066  1.3943  1.7358\n",
      "   0.9585  0.8331  0.9897  1.0527  1.2048\n",
      "   1.1706  1.1882  1.4107  1.3348  0.8901\n",
      "   1.4027  1.5039  1.1898  1.0960  0.7940\n",
      "   0.6166  0.5615  0.2683  0.7654  0.7188\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-04 *\n",
      "  -0.0109 -0.0509 -0.2116 -0.2433 -0.2534\n",
      "   0.1221  0.4495  0.6416  0.5121  0.1890\n",
      "   0.1797  0.4665  0.3108  0.5834  0.7325\n",
      "   0.3454  0.5377  0.2491  0.3548  0.3701\n",
      "   0.2128  0.1419 -0.1605  0.0131  0.2405\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-62:\n",
      "Process Process-61:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/queues.py\", line 392, in put\n",
      "    return send(obj)\n",
      "    return recv()\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 17, in send\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-124857ffae7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-8ed51e1fc0a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mmax_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    217\u001b[0m                ceil_mode=False, return_indices=False):\n\u001b[1;32m    218\u001b[0m     return _functions.thnn.MaxPool2d(kernel_size, stride, padding, dilation,\n\u001b[0;32m--> 219\u001b[0;31m                                      return_indices, ceil_mode)(input)\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/nn/_functions/thnn/pooling.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype2backend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         backend.SpatialDilatedMaxPooling_updateOutput(backend.library_state,\n\u001b[1;32m     74\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mnew\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Constructs a new tensor of the same data type.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 224, in dump\n",
      "    self.save(obj)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 554, in save_tuple\n",
      "    save(element)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 606, in save_list\n",
      "    self._batch_appends(iter(obj))\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 639, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/multiprocessing/forking.py\", line 67, in dispatcher\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 401, in save_reduce\n",
      "    save(args)\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 286, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/Users/vmullachery/anaconda/envs/deeplearn/lib/python2.7/pickle.py\", line 538, in save_tuple\n",
      "    def save_tuple(self, obj):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "params = list(net.parameters())\n",
    "print('len of params: ', len(params))\n",
    "for k in range(len(params)):\n",
    "    print('size of ',  k, ' params: ', params[k].size())\n",
    "    \n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 0:\n",
    "            print('[%5d %5d] loss: %.3f' % (epoch, i, running_loss/2000))\n",
    "            print('conv1 bias grad', list(net.conv1.parameters())[0].grad)\n",
    "            running_loss = 0\n",
    "print('Training complete')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Groundtruth:', '    7     2     1     0')\n",
      "('predicted: ', '    7     2     1     0')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b003850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Groundtruth:', ' '.join('%5s' % labels[j] for j in range(4)))\n",
    "\n",
    "outputs = net(Variable(images))\n",
    "\n",
    "#outputs are the energies for the 10 class indexes\n",
    "_, predictions = torch.max(outputs.data, 1)\n",
    "print('predicted: ', ' '.join('%5s' % predictions[j][0] for j in range(4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98 %  \n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy: %d %%  ' % (100 * correct/total) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " plane, class_correct    37%\n",
      "   car, class_correct    80%\n",
      "  bird, class_correct    36%\n",
      "   cat, class_correct    44%\n",
      "  deer, class_correct    35%\n",
      "   dog, class_correct    52%\n",
      "  frog, class_correct    68%\n",
      " horse, class_correct    57%\n",
      "  ship, class_correct    84%\n",
      " truck, class_correct    42%\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    c = (predictions == labels).squeeze()\n",
    "    #print('c', c)\n",
    "    for i, p in enumerate(predictions):\n",
    "        #print('p: ', p[0])\n",
    "        label = labels[i]\n",
    "        class_total[label] += 1\n",
    "        #class_correct[label] += (label == p[0])\n",
    "        class_correct[label] += (c[i])\n",
    "for i in range(10):\n",
    "    print(' %5s, class_correct %5d%%' % (classes[i], 100*class_correct[i]/class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
